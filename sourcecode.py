# -*- coding: utf-8 -*-
"""Test - Popularity DataSet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FrjFzHnoDRTPn6Z8wSU8cg-8mfEf-JBW
"""

#importing pandas and dataset
import pandas as pd
import numpy as np
import seaborn as sns
news_popularity = pd.read_csv('/content/sample_data/online_news_popularity.csv')


#making a copy of the dataset to leave the original dataset alone
new_data = news_popularity.copy()

#----------------Data Exploration and Cleaning--------------------------------


#removing leading spaces from attribute names
col_names = list(new_data.columns.values)
col_names = [a.strip() for a in col_names]
column_indices = [range(0,61)]
old_names = new_data.columns[column_indices]
new_data.rename(columns=dict(zip(old_names,col_names)),inplace=True)
#len(new_data.index)


#removing columns from the list where the 0's actually mean something and is not considered missing values
cols = list(new_data.columns.values)
unwanted = [9,10,13,14,15,16,17,18,31,32,33,34,35,36,37,38]  
for index in sorted(unwanted, reverse=True):
    del cols[index]

#replacing 0s with nan
new_data[cols] = new_data[cols].replace({0:np.nan})

#making a heatmap to see what columns have the most missing values
sns.heatmap(new_data.isnull(), cbar=False)

#remove first two non-predictable columns
rangeofcol = range(2,61)
new_data = new_data.iloc[:,rangeofcol]

#drop rows with any missing values. Reduces run-time GREATLY without reducing accuracy!!
new_data = new_data.dropna()

#checking for details of 'shares' column to classify as popular or not popular
info = news_popularity[' shares'].describe()

#mean is 3395. More than 3395 = popular. This will help the classification algorithm to do its work
new_data['target_class'] = np.where(new_data['shares']>= 3395, 1, 0)


new_data.head()

#creating independent and dependent variables

from pandas.plotting import scatter_matrix
from matplotlib import cm
from matplotlib import pyplot

x = new_data.drop(['target_class','shares'],axis=1)
y = new_data['target_class']
print(x)
print(y)

#splitting data into train and test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.30,random_state=0)

#normalizing data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Commented out IPython magic to ensure Python compatibility. 
%%time
#decision tree and checking accuracy on train and test data
from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics as sm

clf = DecisionTreeClassifier().fit(x_train,y_train)

#checking the importance of each column. commented so it doesn't run when code is run normally. 
importance = clf.feature_importances_
inorder_imp = []
for i,v in enumerate(importance):
 inorder_imp.append(v)
  print('Feature: %0d, Score: %.5f' % (i,v))



#checking accuracy
print('Accuracy on train data for decision tree: {:.2f}'
  .format(clf.score(x_train,y_train)))
print('Accuracy on test data for decision tree: {:.2f}'
  .format(clf.score(x_test,y_test)))


#Commented out IPython magic to ensure Python compatibility.
%%time
#KNN and checking accuracy on train and test data
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)
print('Accuracy on train data for KNN: {:.2f}'
  .format(knn.score(x_train,y_train)))
print('Accuracy on test data for KNN: {:.2f}'
  .format(knn.score(x_test,y_test)))

#Commented out IPython magic to ensure Python compatibility.
%%time
#SVM and checking accuracy on train and test data
from sklearn.svm import SVC
svm = SVC()
svm.fit(x_train,y_train)
print('Accuracy on train data for SVM: {:.2f}'
  .format(svm.score(x_train,y_train)))
print('Accuracy on test data for SVM: {:.2f}'
  .format(svm.score(x_test,y_test)))

#Confusion Matrix for SVM
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
pred = svm.predict(x_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
